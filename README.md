# ğŸ¤– AI Document Assistant Using Llama3
Your intelligent document analysis companion powered by LLaMA 3 and advanced RAG (Retrieval-Augmented Generation) technology.

![image](https://github.com/user-attachments/assets/2e533af5-cc31-4393-a9fc-0ae215c924db)
![image](https://github.com/user-attachments/assets/e70fa1a9-2b9f-43d9-a348-1a6eec5fc319)

## ğŸ“– Overview
- The AI Document Assistant is a Streamlit-based web application that allows users to upload PDF documents and interact with them via natural language queries. Powered by LLaMA 3 and advanced RAG techniques, it provides accurate, context-aware responses, making it perfect for researchers, professionals, and students.
  
## âœ¨ Features

- **ğŸ“„ Multi-PDF Upload**: Process multiple PDF files simultaneously.
- **ğŸ” Smart Search**: Vector-based retrieval using FAISS for precise results.
- **ğŸ’¬ Conversational Q&A**: Ask questions in natural language and get detailed answers.
- **ğŸ“š Source Attribution**: View document sources and page numbers for transparency.
- **ğŸ’¾ Conversation History**: Save and review chat history with timestamps.
- **ğŸ¨ Sleek UI**: Responsive, modern interface with custom styling.
- **âš™ Customizable**: Adjust embedding models, system prompts, and processing settings.

## ğŸ› ï¸ Technology Stack

- **Frontend**: Streamlit 1.38.0
- **LLM**: LLaMA 3 (via Ollama)
- **Vector Database**: FAISS
- **Document Processing**: PyPDF2 3.0.1
- **Text Chunking**: LangChain RecursiveCharacterTextSplitter
- **Embeddings**: HuggingFace (all-MiniLM-L6-v2 or all-distilroberta-v1)
- **Framework**: LangChain 0.3.1, LangChain-Community 0.3.1, LangChain-Ollama 0.2.0

## ğŸ“¦ Installation
### Prerequisites

- ğŸ Python 3.8+ installed
- ğŸ¦™ Ollama installed and running locally
- ğŸ¤– LLaMA 3 model pulled in Ollama

### Setup

1. **Install Ollama**
   Download and install Ollama from the official website for your operating system.
2. **Pull LLaMA 3 Model**
   ollama pull llama3

3. **Clone the Repository**
   ```bash
   git clone https://github.com/Jasvinder21/AI-Document-Assistant-Using-Llama3
   cd AI-Document-Assistant-Using-Llama3
   ```

4. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
**Notes**:Install FAISS separately (not included in requirements.txt):
   ```bach
   pip install faiss-cpu==1.8.0
   ```

## ğŸ“‹ Requirements

Create a `requirements.txt` file with the following dependencies:

```txt
streamlit==1.38.0
PyPDF2==3.0.1
langchain==0.3.1
langchain-community==0.3.1
langchain-ollama==0.2.0
ollama==0.3.3
sentence-transformers==3.1.1
numpy==1.26.4
faiss-cpu==1.8.0
```

## ğŸš€ Usage
1. **Starting the Application:**
   ```bash
   Start Ollama Server:
   ollama serve
   ```

2. **Run the Streamlit App:**
   ```bash
   streamlit run app.py
   ```

3. **Access the App:**
   Open your browser and navigate to http://localhost:8501.
   

## Using the Application

- **ğŸ“¤ Upload PDFs**: Use the sidebar to upload one or more PDF files.
- **ğŸ”„ Process Documents**: Click "Process Documents" to generate vector embeddings.
- **â“ Ask Questions**: Enter queries in the chat interface.
- **ğŸ“œ View Responses**: Get AI-generated answers with source document and page references.
- **ğŸ•°ï¸ Review History**: Expand the conversation history to see past interactions.

## âš™ï¸ Configuration
### System Prompt
  Customize the AI's behavior via the sidebar's system prompt. The default prompt ensures           clear, accurate responses while acknowledging uncertainties.
   

### Processing Parameters

- **Chunk Size**: 500 characters
- **Chunk Overlap**: 50 characters
- **Retrieved Documents**: 3 relevant chunks per query

  ### Model Configuration

- **Embedding Models**: all-MiniLM-L6-v2 or all-distilroberta-v1
- **LLM Model**: LLaMA 3
- **Vector Store**: FAISS, stored locally in data/vectorstore/my_store

## ğŸ“ Project Structure
```
AI-Document-Assistant-Using-Llama3/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # Project documentation
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vectorstore/
â”‚       â””â”€â”€ my_store/     # FAISS vector store and conversation history
â””â”€â”€ output/
    â”œâ”€â”€ output1.png       # Sample screenshot
    â”œâ”€â”€ output2.png       # Sample screenshot
    â”œâ”€â”€ output3.png       # Sample screenshot
    â””â”€â”€ output4.mp4       # Demo video
```

## ğŸ”§ Key Functions

- **read_data()**: Extracts text from PDFs using PyPDF2.
- **get_chunks()**: Splits documents into chunks for processing.
- **vector_store()**: Creates and saves FAISS vector embeddings.
- **load_vector_store()**: Loads the vector store for querying.
- **user_input()**: Handles user queries and displays responses with sources.
- **get_conversational_chain()**: Builds the RAG chain for Q&A.

## ğŸ¯ Use Cases

- **ğŸ”¬ Research Analysis**: Extract insights from academic papers or reports.
- **ğŸ“ Document Summarization**: Summarize lengthy documents quickly.
- **âš–ï¸ Legal Review**: Query specific clauses or terms in contracts.
- **ğŸ“š Technical Docs**: Find procedures or requirements in manuals.
- **ğŸ“ Educational Tools**: Study and quiz from textbooks or notes.

## ğŸ”’ Privacy & Security

- **ğŸ–¥ï¸ Local Processing**: All operations run locally.
- **ğŸ” No External Sharing**: Documents and queries stay on your device (unless Ollama is configured remotely).
- **ğŸ’¾ Local Storage**: Vector embeddings and history are stored in data/.
- **ğŸ›¡ï¸ Secure Embeddings**: FAISS ensures efficient and secure vector storage.

## ğŸ¤ Contributing
 Contributions are welcome! To contribute:

1. ğŸ´ Fork the repository.
2. ğŸŒ¿ Create a feature branch (git checkout -b feature/amazing-feature).
3. ğŸ’¾ Commit your changes (git commit -m 'Add amazing feature').
4. ğŸš€ Push to the branch (git push origin feature/amazing-feature).
5. ğŸ“¬ Open a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License.
ğŸ™ Acknowledgments

Ollama for local LLM infrastructure.
LangChain for RAG and text processing.
Streamlit for the web interface.
HuggingFace for embedding models.
Facebook AI Research for FAISS.

## ğŸ“ Support
For issues or questions:

1. Visit the Issues page.
2. Create a new issue with detailed information.
3. Ensure Ollama is running and LLaMA 3 is installed (ollama pull llama3).

## ğŸš§ Roadmap

 Support for additional formats (DOCX, TXT).
 Multi-LLM model support.
 Advanced chunking and retrieval strategies.

 Document comparison and summarization tools.
 Exportable conversation history.
 Docker containerization.
 Cloud deployment options.


**Built with ğŸ–¥ï¸ Streamlit & LangChain | Powered by ğŸ¦™ LLaMA 3**
